{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, we'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crivera5/.virtualenvs/aind/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network.\n",
    "\n",
    "To spice it up I also added War and Peace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "    \n",
    "with open('WarAndPeace.txt', 'r') as f:\n",
    "    text=text + f.read()   \n",
    "    \n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32, 65, 58, 73, 77, 62, 75,  1, 16,  0,  0,  0, 37, 58, 73, 73, 82,\n",
       "        1, 63, 58, 70, 66, 69, 66, 62, 76,  1, 58, 75, 62,  1, 58, 69, 69,\n",
       "        1, 58, 69, 66, 68, 62, 26,  1, 62, 79, 62, 75, 82,  1, 78, 71, 65,\n",
       "       58, 73, 73, 82,  1, 63, 58, 70, 66, 69, 82,  1, 66, 76,  1, 78, 71,\n",
       "       65, 58, 73, 73, 82,  1, 66, 71,  1, 66, 77, 76,  1, 72, 80, 71,  0,\n",
       "       80, 58, 82, 13,  0,  0, 34, 79, 62, 75, 82, 77, 65, 66, 71],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the other data structures introduced above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches\n",
    "\n",
    "Here is where we'll make our mini-batches for training. Remember that we want our batches to be multiple sequences of some desired number of sequence steps. Considering a simple example, our batches would look like this:\n",
    "\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "We start with our text encoded as integers in one long array in `encoded`. Let's create a function that will give us an iterator for our batches. I like using [generator functions](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/) to do this. Then we can pass `encoded` into this function and get our batch generator.\n",
    "\n",
    "The first thing we need to do is discard some of the text so we only have completely full batches. Each batch contains $N \\times M$ characters, where $N$ is the batch size (the number of sequences) and $M$ is the number of steps. Then, to get the total number of batches, $K$, we can make from the array `arr`, you divide the length of `arr` by the number of characters per batch. Once you know the number of batches, you can get the total number of characters to keep from `arr`, $N * M * K$.\n",
    "\n",
    "After that, we need to split `arr` into $N$ sequences. You can do this using `arr.reshape(size)` where `size` is a tuple containing the dimensions sizes of the reshaped array. We know we want $N$ sequences (`batch_size` below), let's make that the size of the first dimension. For the second dimension, you can use `-1` as a placeholder in the size, it'll fill up the array with the appropriate data for you. After this, you should have an array that is $N \\times (M * K)$.\n",
    "\n",
    "Now that we have this array, we can iterate through it to get our batches. The idea is each batch is a $N \\times M$ window on the $N \\times (M * K)$ array. For each subsequent batch, the window moves over by `n_steps`. We also want to create both the input and target arrays. Remember that the targets are the inputs shifted over one character. \n",
    "\n",
    "The way I like to do this window is use `range` to take steps of size `n_steps` from $0$ to `arr.shape[1]`, the total number of steps in each sequence. That way, the integers you get from `range` always point to the start of a batch, and each window is `n_steps` wide.\n",
    "\n",
    "> **Exercise:** Write the code for creating batches in the function below. The exercises in this notebook _will not be easy_. I've provided a notebook with solutions alongside this notebook. If you get stuck, checkout the solutions. The most important thing is that you don't copy and paste the code into here, **type out the solution code yourself.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the number of characters per batch and number of batches we can make\n",
    "    chars_per_batch= batch_size * n_steps\n",
    "    n_batches = len(arr)//chars_per_batch\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:chars_per_batch*n_batches]\n",
    "    \n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    arr_y = np.roll(arr, -1)\n",
    "\n",
    "    # iterate thought the array\n",
    "    for n in range(0, arr.shape[1], n_steps): \n",
    "        x = arr[:, n:n+n_steps]\n",
    "        y = arr_y[:, n:n+n_steps]\n",
    "        \n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x,y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[32 65 58 73 77 62 75  1 16  0]\n",
      " [65  1 77 65 62  1 73 62 77 58]\n",
      " [72 78 69 61  1 71 72 77  1 69]\n",
      " [83 61 79 66 83 65 62 71 76 68]\n",
      " [ 1 62 79 62 71  1 65 62  1 80]\n",
      " [79 62  1 78 76 62 63 78 69  1]\n",
      " [65 62  1 65 72 71 62 76 77  1]\n",
      " [71 72 77  1 58  0 77 75 58 60]\n",
      " [59 62 62 71  1 58  1 75 62 79]\n",
      " [60 75 66 59 62 61  1 77 72  1]]\n",
      "\n",
      "y\n",
      " [[65 58 73 77 62 75  1 16  0  0]\n",
      " [ 1 77 65 62  1 73 62 77 58 69]\n",
      " [78 69 61  1 71 72 77  1 69 62]\n",
      " [61 79 66 83 65 62 71 76 68 72]\n",
      " [62 79 62 71  1 65 62  1 80 58]\n",
      " [62  1 78 76 62 63 78 69  1 77]\n",
      " [62  1 65 72 71 62 76 77  1 70]\n",
      " [72 77  1 58  0 77 75 58 60 62]\n",
      " [62 62 71  1 58  1 75 62 79 72]\n",
      " [75 66 59 62 61  1 77 72  1 78]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented `get_batches` correctly, the above output should look something like \n",
    "```\n",
    "x\n",
    " [[55 63 69 22  6 76 45  5 16 35]\n",
    " [ 5 69  1  5 12 52  6  5 56 52]\n",
    " [48 29 12 61 35 35  8 64 76 78]\n",
    " [12  5 24 39 45 29 12 56  5 63]\n",
    " [ 5 29  6  5 29 78 28  5 78 29]\n",
    " [ 5 13  6  5 36 69 78 35 52 12]\n",
    " [63 76 12  5 18 52  1 76  5 58]\n",
    " [34  5 73 39  6  5 12 52 36  5]\n",
    " [ 6  5 29 78 12 79  6 61  5 59]\n",
    " [ 5 78 69 29 24  5  6 52  5 63]]\n",
    "\n",
    "y\n",
    " [[63 69 22  6 76 45  5 16 35 35]\n",
    " [69  1  5 12 52  6  5 56 52 29]\n",
    " [29 12 61 35 35  8 64 76 78 28]\n",
    " [ 5 24 39 45 29 12 56  5 63 29]\n",
    " [29  6  5 29 78 28  5 78 29 45]\n",
    " [13  6  5 36 69 78 35 52 12 43]\n",
    " [76 12  5 18 52  1 76  5 58 52]\n",
    " [ 5 73 39  6  5 12 52 36  5 78]\n",
    " [ 5 29 78 12 79  6 61  5 59 63]\n",
    " [78 69 29 24  5  6 52  5 63 76]]\n",
    " ```\n",
    " although the exact numbers will be different. Check to make sure the data is shifted over one step for `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Below is where you'll build the network. We'll break it up into parts so it's easier to reason about each bit. Then we can connect them up into the whole network.\n",
    "\n",
    "<img src=\"assets/charRNN.png\" width=500px>\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "First off we'll create our input placeholders. As usual we need placeholders for the training data and the targets. We'll also create a placeholder for dropout layers called `keep_prob`. This will be a scalar, that is a 0-D tensor. To make a scalar, you create a placeholder without giving it a size.\n",
    "\n",
    "> **Exercise:** Create the input placeholders in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size: Batch size, number of sequences per batch\n",
    "        num_steps: Number of sequence steps in a batch\n",
    "        \n",
    "    '''\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, shape = (batch_size, num_steps))\n",
    "    targets = tf.placeholder(tf.int32, shape = (batch_size, num_steps))\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32,  name = 'keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell\n",
    "\n",
    "Here we will create the LSTM cell we'll use in the hidden layer. We'll use this cell as a building block for the RNN. So we aren't actually defining the RNN here, just the type of cell we'll use in the hidden layer.\n",
    "\n",
    "We first create a basic LSTM cell with\n",
    "\n",
    "```python\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "```\n",
    "\n",
    "where `num_units` is the number of units in the hidden layers in the cell. Then we can add dropout by wrapping it with \n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "```\n",
    "You pass in a cell and it will automatically add dropout to the inputs or outputs. Finally, we can stack up the LSTM cells into layers with [`tf.contrib.rnn.MultiRNNCell`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/MultiRNNCell). With this, you pass in a list of cells and it will send the output of one cell into the next cell. Previously with TensorFlow 1.0, you could do this\n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.MultiRNNCell([cell]*num_layers)\n",
    "```\n",
    "\n",
    "This might look a little weird if you know Python well because this will create a list of the same `cell` object. However, TensorFlow 1.0 will create different weight matrices for all `cell` objects. But, starting with TensorFlow 1.1 you actually need to create new cell objects in the list. To get it to work in TensorFlow 1.1, it should look like\n",
    "\n",
    "```python\n",
    "def build_cell(num_units, keep_prob):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    return drop\n",
    "    \n",
    "tf.contrib.rnn.MultiRNNCell([build_cell(num_units, keep_prob) for _ in range(num_layers)])\n",
    "```\n",
    "\n",
    "Even though this is actually multiple LSTM cells stacked on each other, you can treat the multiple layers as one cell.\n",
    "\n",
    "We also need to create an initial cell state of all zeros. This can be done like so\n",
    "\n",
    "```python\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "```\n",
    "\n",
    "Below, we implement the `build_lstm` function to create these LSTM cells and the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob, cell_type= None):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Build the LSTM Cell\n",
    "    # Use a basic LSTM cell\n",
    "    if cell_type is None: \n",
    "        cell_type = tf.contrib.rnn.BasicLSTMCell\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        lstm = cell_type(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, keep_prob)\n",
    "        return drop\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Output\n",
    "\n",
    "Here we'll create the output layer. We need to connect the output of the RNN cells to a full connected layer with a softmax output. The softmax output gives us a probability distribution we can use to predict the next character, so we want this layer to have size $C$, the number of classes/characters we have in our text.\n",
    "\n",
    "If our input has batch size $N$, number of steps $M$, and the hidden layer has $L$ hidden units, then the output is a 3D tensor with size $N \\times M \\times L$. The output of each LSTM cell has size $L$, we have $M$ of them, one for each sequence step, and we have $N$ sequences. So the total size is $N \\times M \\times L$. \n",
    "\n",
    "We are using the same fully connected layer, the same weights, for each of the outputs. Then, to make things easier, we should reshape the outputs into a 2D tensor with shape $(M * N) \\times L$. That is, one row for each sequence and step, where the values of each row are the output from the LSTM cells. We get the LSTM output as a list, `lstm_output`. First we need to concatenate this whole list into one array with [`tf.concat`](https://www.tensorflow.org/api_docs/python/tf/concat). Then, reshape it (with `tf.reshape`) to size $(M * N) \\times L$.\n",
    "\n",
    "One we have the outputs reshaped, we can do the matrix multiplication with the weights. We need to wrap the weight and bias variables in a variable scope with `tf.variable_scope(scope_name)` because there are weights being created in the LSTM cells. TensorFlow will throw an error if the weights created here have the same names as the weights created in the LSTM cells, which they will be default. To avoid this, we wrap the variables in a variable scope so we can give them unique names.\n",
    "\n",
    "> **Exercise:** Implement the output layer in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        lstm_output: List of output tensors from the LSTM layer\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # Concatenate lstm_output over axis 1 (the columns)\n",
    "    seq_output = tf.concat( lstm_output, axis = 1)\n",
    "    # Reshape seq_output to a 2D tensor with lstm_size columns\n",
    "    x = tf.reshape(seq_output, shape = (-1, in_size))\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        # Create the weight and bias variables here\n",
    "        softmax_w = tf.get_variable('w', shape = (in_size, out_size), initializer = tf.truncated_normal_initializer)\n",
    "        softmax_b = tf.get_variable('b', shape =out_size, initializer=tf.zeros_initializer)\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.nn.xw_plus_b(x, softmax_w, softmax_b)\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits ,name='logits')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss\n",
    "\n",
    "Next up is the training loss. We get the logits and targets and calculate the softmax cross-entropy loss. First we need to one-hot encode the targets, we're getting them as encoded characters. Then, reshape the one-hot targets so it's a 2D tensor with size $(M*N) \\times C$ where $C$ is the number of classes/characters we have. Remember that we reshaped the LSTM outputs and ran them through a fully connected layer with $C$ units. So our logits will also have size $(M*N) \\times C$.\n",
    "\n",
    "Then we run the logits and targets through `tf.nn.softmax_cross_entropy_with_logits` and find the mean to get the loss.\n",
    "\n",
    ">**Exercise:** Implement the loss calculation in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hot encode targets and reshape to match logits, one row per sequence per step\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped =  tf.reshape(y_one_hot, shape = logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y_reshaped)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Here we build the optimizer. Normal RNNs have have issues gradients exploding and disappearing. LSTMs fix the disappearance problem, but the gradients can still grow without bound. To fix this, we can clip the gradients above some threshold. That is, if a gradient is larger than that threshold, we set it to the threshold. This will ensure the gradients never grow overly large. Then we use an AdamOptimizer for the learning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network\n",
    "\n",
    "Now we can put all the pieces together and build a class for the network. To actually run data through the LSTM cells, we will use [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn). This function will pass the hidden and cell states across LSTM cells appropriately for us. It returns the outputs for each LSTM cell at each step for each sequence in the mini-batch. It also gives us the final LSTM state. We want to save this state as `final_state` so we can pass it to the first LSTM cell in the the next mini-batch run. For `tf.nn.dynamic_rnn`, we pass in the cell and initial state we get from `build_lstm`, as well as our input sequences. Also, we need to one-hot encode the inputs before going into the RNN. \n",
    "\n",
    "> **Exercise:** Use the functions you've implemented previously and `tf.nn.dynamic_rnn` to build the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False, cell_type=None):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob, cell_type)\n",
    "\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN with tf.nn.dynamic_rnn \n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, inputs=x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss =  build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer =  build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here are the hyperparameters for the network.\n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to copy it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50       # Sequences per batch\n",
    "num_steps = 50         # Number of sequence steps per batch\n",
    "lstm_size = 128         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.01    # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for training\n",
    "\n",
    "This is typical training code, passing inputs and targets into the network, then running the optimizer. Here we also get back the final LSTM state for the mini-batch. Then, we pass that state back into the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}.ckpt`\n",
    "\n",
    "> **Exercise:** Set the hyperparameters above to train the network. Watch the training loss, it should be consistently dropping. Also, I highly advise running this on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/crivera5/.virtualenvs/aind/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-12-1b64abfc27d9>:18: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Epoch: 1/50... Training Step:...500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 1/50... Training Step:...1000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 1/50... Training Step:...1500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 1/50... Training Step:...2000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 2/50... Training Step:...2500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 2/50... Training Step:...3000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 2/50... Training Step:...3500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 2/50... Training Step:...4000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 3/50... Training Step:...4500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 3/50... Training Step:...5000  Training loss:...2.0 0.07 sec/batch\n",
      "Epoch: 3/50... Training Step:...5500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 3/50... Training Step:...6000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 4/50... Training Step:...6500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 4/50... Training Step:...7000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 4/50... Training Step:...7500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 4/50... Training Step:...8000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 5/50... Training Step:...8500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 5/50... Training Step:...9000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 5/50... Training Step:...9500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 5/50... Training Step:...10000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 6/50... Training Step:...10500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 6/50... Training Step:...11000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 6/50... Training Step:...11500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 6/50... Training Step:...12000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 7/50... Training Step:...12500  Training loss:...2.0 0.07 sec/batch\n",
      "Epoch: 7/50... Training Step:...13000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 7/50... Training Step:...13500  Training loss:...2.0 0.07 sec/batch\n",
      "Epoch: 7/50... Training Step:...14000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 7/50... Training Step:...14500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 8/50... Training Step:...15000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 8/50... Training Step:...15500  Training loss:...2.0 0.07 sec/batch\n",
      "Epoch: 8/50... Training Step:...16000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 8/50... Training Step:...16500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 9/50... Training Step:...17000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 9/50... Training Step:...17500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 9/50... Training Step:...18000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 9/50... Training Step:...18500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 10/50... Training Step:...19000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 10/50... Training Step:...19500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 10/50... Training Step:...20000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 10/50... Training Step:...20500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 11/50... Training Step:...21000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 11/50... Training Step:...21500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 11/50... Training Step:...22000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 11/50... Training Step:...22500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 12/50... Training Step:...23000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 12/50... Training Step:...23500  Training loss:...2.0 0.07 sec/batch\n",
      "Epoch: 12/50... Training Step:...24000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 12/50... Training Step:...24500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 13/50... Training Step:...25000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 13/50... Training Step:...25500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 13/50... Training Step:...26000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 13/50... Training Step:...26500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 13/50... Training Step:...27000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 14/50... Training Step:...27500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 14/50... Training Step:...28000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 14/50... Training Step:...28500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 14/50... Training Step:...29000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 15/50... Training Step:...29500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 15/50... Training Step:...30000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 15/50... Training Step:...30500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 15/50... Training Step:...31000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 16/50... Training Step:...31500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 16/50... Training Step:...32000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 16/50... Training Step:...32500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 16/50... Training Step:...33000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 17/50... Training Step:...33500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 17/50... Training Step:...34000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 17/50... Training Step:...34500  Training loss:...2.0 0.07 sec/batch\n",
      "Epoch: 17/50... Training Step:...35000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 18/50... Training Step:...35500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 18/50... Training Step:...36000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 18/50... Training Step:...36500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 18/50... Training Step:...37000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 19/50... Training Step:...37500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 19/50... Training Step:...38000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 19/50... Training Step:...38500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 19/50... Training Step:...39000  Training loss:...2.0 0.08 sec/batch\n",
      "Epoch: 20/50... Training Step:...39500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 20/50... Training Step:...40000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 20/50... Training Step:...40500  Training loss:...2.0 0.07 sec/batch\n",
      "Epoch: 20/50... Training Step:...41000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 20/50... Training Step:...41500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 21/50... Training Step:...42000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 21/50... Training Step:...42500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 21/50... Training Step:...43000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 21/50... Training Step:...43500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 22/50... Training Step:...44000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 22/50... Training Step:...44500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 22/50... Training Step:...45000  Training loss:...2.0 0.07 sec/batch\n",
      "Epoch: 22/50... Training Step:...45500  Training loss:...2.0 0.07 sec/batch\n",
      "Epoch: 23/50... Training Step:...46000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 23/50... Training Step:...46500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 23/50... Training Step:...47000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 23/50... Training Step:...47500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 24/50... Training Step:...48000  Training loss:...2.0 0.05 sec/batch\n",
      "Epoch: 24/50... Training Step:...48500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 24/50... Training Step:...49000  Training loss:...2.0 0.08 sec/batch\n",
      "Epoch: 24/50... Training Step:...49500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 25/50... Training Step:...50000  Training loss:...2.0 0.06 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25/50... Training Step:...50500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 25/50... Training Step:...51000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 25/50... Training Step:...51500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 26/50... Training Step:...52000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 26/50... Training Step:...52500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 26/50... Training Step:...53000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 26/50... Training Step:...53500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 26/50... Training Step:...54000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 27/50... Training Step:...54500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 27/50... Training Step:...55000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 27/50... Training Step:...55500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 27/50... Training Step:...56000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 28/50... Training Step:...56500  Training loss:...2.0 0.07 sec/batch\n",
      "Epoch: 28/50... Training Step:...57000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 28/50... Training Step:...57500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 28/50... Training Step:...58000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 29/50... Training Step:...58500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 29/50... Training Step:...59000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 29/50... Training Step:...59500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 29/50... Training Step:...60000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 30/50... Training Step:...60500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 30/50... Training Step:...61000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 30/50... Training Step:...61500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 30/50... Training Step:...62000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 31/50... Training Step:...62500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 31/50... Training Step:...63000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 31/50... Training Step:...63500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 31/50... Training Step:...64000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 32/50... Training Step:...64500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 32/50... Training Step:...65000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 32/50... Training Step:...65500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 32/50... Training Step:...66000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 33/50... Training Step:...66500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 33/50... Training Step:...67000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 33/50... Training Step:...67500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 33/50... Training Step:...68000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 33/50... Training Step:...68500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 34/50... Training Step:...69000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 34/50... Training Step:...69500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 34/50... Training Step:...70000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 34/50... Training Step:...70500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 35/50... Training Step:...71000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 35/50... Training Step:...71500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 35/50... Training Step:...72000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 35/50... Training Step:...72500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 36/50... Training Step:...73000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 36/50... Training Step:...73500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 36/50... Training Step:...74000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 36/50... Training Step:...74500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 37/50... Training Step:...75000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 37/50... Training Step:...75500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 37/50... Training Step:...76000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 37/50... Training Step:...76500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 38/50... Training Step:...77000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 38/50... Training Step:...77500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 38/50... Training Step:...78000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 38/50... Training Step:...78500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 39/50... Training Step:...79000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 39/50... Training Step:...79500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 39/50... Training Step:...80000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 39/50... Training Step:...80500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 39/50... Training Step:...81000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 40/50... Training Step:...81500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 40/50... Training Step:...82000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 40/50... Training Step:...82500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 40/50... Training Step:...83000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 41/50... Training Step:...83500  Training loss:...2.0 0.07 sec/batch\n",
      "Epoch: 41/50... Training Step:...84000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 41/50... Training Step:...84500  Training loss:...2.0 0.07 sec/batch\n",
      "Epoch: 41/50... Training Step:...85000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 42/50... Training Step:...85500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 42/50... Training Step:...86000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 42/50... Training Step:...86500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 42/50... Training Step:...87000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 43/50... Training Step:...87500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 43/50... Training Step:...88000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 43/50... Training Step:...88500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 43/50... Training Step:...89000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 44/50... Training Step:...89500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 44/50... Training Step:...90000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 44/50... Training Step:...90500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 44/50... Training Step:...91000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 45/50... Training Step:...91500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 45/50... Training Step:...92000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 45/50... Training Step:...92500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 45/50... Training Step:...93000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 46/50... Training Step:...93500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 46/50... Training Step:...94000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 46/50... Training Step:...94500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 46/50... Training Step:...95000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 46/50... Training Step:...95500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 47/50... Training Step:...96000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 47/50... Training Step:...96500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 47/50... Training Step:...97000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 47/50... Training Step:...97500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 48/50... Training Step:...98000  Training loss:...2.0 0.07 sec/batch\n",
      "Epoch: 48/50... Training Step:...98500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 48/50... Training Step:...99000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 48/50... Training Step:...99500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 49/50... Training Step:...100000  Training loss:...2.0 0.07 sec/batch\n",
      "Epoch: 49/50... Training Step:...100500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 49/50... Training Step:...101000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 49/50... Training Step:...101500  Training loss:...2.0 0.07 sec/batch\n",
      "Epoch: 50/50... Training Step:...102000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 50/50... Training Step:...102500  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 50/50... Training Step:...103000  Training loss:...2.0 0.06 sec/batch\n",
      "Epoch: 50/50... Training Step:...103500  Training loss:...2.0 0.06 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "# Print losses every N interations\n",
    "print_every_n = 500\n",
    "\n",
    "# Save every N iterations\n",
    "save_every_n = 1000\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            if (counter % print_every_n == 0):\n",
    "                end = time.time()\n",
    "                print('Epoch: {}/{}...'.format(e+1, epochs),\n",
    "                      'Training Step:...{} '.format(counter),\n",
    "                      'Training loss:...{}'.format(np.round(np.mean(batch_loss)),4),\n",
    "                      '{} sec/batch'.format(np.round(end-start,2)) \n",
    "                     )     \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i103850_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i5000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i6000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i7000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i8000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i9000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i10000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i11000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i12000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i13000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i14000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i15000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i16000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i17000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i18000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i19000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i20000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i21000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i22000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i23000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i24000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i25000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i26000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i27000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i28000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i29000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i30000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i31000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i32000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i33000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i34000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i35000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i36000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i37000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i38000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i39000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i40000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i41000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i42000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i43000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i44000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i45000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i46000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i47000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i48000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i49000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i50000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i51000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i52000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i53000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i54000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i55000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i56000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i57000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i58000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i59000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i60000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i61000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i62000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i63000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i64000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i65000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i66000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i67000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i68000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i69000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i70000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i71000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i72000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i73000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i74000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i75000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i76000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i77000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i78000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i79000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i80000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i81000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i82000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i83000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i84000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i85000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i86000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i87000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i88000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i89000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i90000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i91000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i92000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i93000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i94000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i95000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i96000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i97000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i98000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i99000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i100000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i101000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i102000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i103000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i103850_l128.ckpt\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "    \n",
    "    samples =['\\n'] + samples\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/i103850_l128.ckpt'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i103850_l128.ckpt\n",
      "\n",
      "Far Far Away at the poot fation. The such arreng world with the rilling of serpacting. But to the forents throes his peoms whal tho frante and thit he sat that he cauld been thin he was\n",
      "ander had some that her feclod and ansuered\n",
      "heads in the care and without told of toe munt of\n",
      "a samp in Mashon hid heard and had there as the chord that they set her to their fronther hould\n",
      "his facling of a sortations was suce half in the reased, and hos was alm and when the\n",
      "surman and how those was altured to the faisent, surmined and the paint and\n",
      "the sumpants, and to her to be somided.\n",
      "\n",
      "\n",
      "This sardes and the riss are and troining him,\n",
      "who had been ano have been arearing her as the sure wore after a porst aroused.\n",
      "\n",
      "“Well, you hore,.\"\n",
      "\n",
      "“Oh, there's all so those to the farsh of the pronce on the priscess, at the muss in\n",
      "intructeole to told the prince ald the frights, and at\n",
      "the reclion of his prsited of the rose of the sire with her fithers on the sormess of actune while the sale\n",
      "was saring a meaten who had been toun then trained on\n",
      "a frestion, and had hart hore were searding one the some of the regumnision and at her.\n",
      "\n",
      "\"I strating the surmer is he seared that his wares to be never he want of atherition of any wint on the string and seeming aland and with ancondestible, but he wisl been that her\n",
      "shiply and had hid ont tried.\n",
      "\n",
      "\"Why, I have you keng to the child of me, In this wout hom that Io yourse so and then\n",
      "the coultess, and insenchited is the stande of her, but he wished and stricher.\n",
      "\n",
      "\"I say you then it want off in the fallion?” asked Perichin, and\n",
      "she saw the sort wene out to bering the\n",
      "prosed and struighed that ho has so another, and there was to think of the surver war the freeded on such a monts, and then the wools to the\n",
      "preach and histering on a sharts of this waild and said at the rars,\n",
      "thay the such the provit arese hore of their strent of an to see hard her though and saw is tronding and ansuer though his\n",
      "funite wish and wish the survers of\n",
      "a mere on a ferents with the pirs ou hossentic somile of them, astened ther thay all all hos had said at his head in the\n",
      "pirinites their was. “It a at the froed see he was\n",
      "imponcy on their pose of and they was stating the poor, so in holes whe was not trathing\n",
      "his stipp of the croan and\n",
      "taked hos whre in a part whome with her solls, while\n",
      "ho sadd him as the sont of the startat the wise the fores of the prince\n",
      "of cheirsed and with ussed to the sormicion of the foot, hed his fashor to sard the farst armion of the prince sainent, so a doos, and all her face was the\n",
      "pastain without highing\n",
      "hustiant who had torn how it was now surplen to answered to his and without said\n",
      "and to streng to she said that they’ded of anything had alt the chold wene.\n",
      "\n",
      "Those was now aldering his fried shite, but at the rilen of the stant that he said.\n",
      "\n",
      "\"To se te seem his astent, the samv of ther, thing as he wish hom,\n",
      "a dress arenced to another, and hall as those already\n",
      "armed or a pallarians, and to the such hallow tarker, he was seening. The pouse was insenting that she was in the count if all a semmotion with her such the sume in\n",
      "the foot af all showed it the remander,\n",
      "bet in a pointed home the farst of her hors, she souded in\n",
      "the frinces is him ho wish at the\n",
      "calk the sores on the for the partl ard of the room. The firse of sharting of toe chald or the famil walled\n",
      "to him to be all, asker over his farler and soldenie toranding a chating of intorder.\n",
      "\n",
      "“There was atred a croan, and then they is so all you whin to se now tiar to hale it, and If the paince of the soul of me with hand and the pisented on the sons of thrier or sorver of the\n",
      "such a fant at the same of his all, ask on a seetents, truing all toe people of\n",
      "too felt were interss in Moscow, but stracg his anginers of serings with a marely, as the\n",
      "fortled to anything the sent of though his honst so sample and\n",
      "shringen of him for the postors and armes in\n",
      "allow how he did not compleen the stors of she wents and so trongly were,\n",
      "and that tairs wish taired a sumverieted of that ho wos her the fress word after him\n",
      "siding tion, and the peasant which wish over their count, sening to the streace\n",
      "tren tre she walked into her face at a sumproson of a count of the surverss. The cornition at this shidit of them on the\n",
      "roud ol him, the whrne was to soote to his\n",
      "anger tire, so she should no heard than he so at a finel some that he was sering to arring hom and settle at a shore, the stoud\n",
      "after this strent of the roms, thar whel soe was in outence aross how\n",
      "soen tied alow his firlt of and the some world or thrigh it talked to the remindar throw the caunt and which had noticed in the suches in a surmert then harrysed the sarence\n",
      "of a solden of the pearer and seand of and somperin the resount of the facler that\n",
      "was set her how with a soldecs war his hone of her hade that he was sectering and\n",
      "the pount of his ammoline of murt and\n",
      "had seared that he can been the rais of the firs of tiss in halk hos\n",
      "army, so it ander hem, though she had sarit of them to these sermet wor her an too and had becanved to his antry. He was a pirenter, all tru harry and\n",
      "has a santean afferent who could net beeang him. But wose the rignen whone stently\n",
      "the officer af sering him and all, all soo said at him\n",
      "to him. Bitúzhv wished out inforde and,\n",
      "to him, see tarned himself.\n",
      "\n",
      "“You seel as tho might whet I huse it the collee, asterst the sume to the fact aftarming and the pronce\n",
      "on are the farle was stard at a mond who were is hear as a sements and to to the command room. The\n",
      "conterious, and hursiaged himself, but the cartsit of the chursed shrustent of the room word into his higter, baliting, with\n",
      "her fate that strengen an the compresting tare then toe pase had\n",
      "aroud to bettle... talk and thire as the order had been im a faces\n",
      "of instat trater to him. She was allowed the stranges to the cumblish, a munsed ouf to say\n",
      "whit was netion,\n",
      "with the room they, wore that shems the persancice of her and\n",
      "sen in the soldee on such accurting and at the coussrions’whole the poom of his seenes, was a mest ancrass of the paling of till on the preach of his fact\n",
      "of a mare are the partle, whree, his whole that they seoked hisself. Toe word, she dod, became at his artation,\n",
      "asked and wert andeading and the from the rain and with the peomeds wents but were to the sorentss on the poss in the\n",
      "forthe were arint hosse the paintions was at his fire on the carmaight at the survist of shill to the perister, and he had become her\n",
      "to tant his arry that they coned thems who had been in the pronte sarts the prince on socong her and\n",
      "all the same too sare toat he dad he had this hond ho wire\n",
      "toes army treed to the stre oun toon the conners, the sare\n",
      "so seree as it tait and toe took of tie wook of all his start to him, the could soon har ouf of the cancs of the comfleted of the crosen, and\n",
      "suddrance offiral santle thit the whole and along was\n",
      "setcly,\n",
      "stilled, stratgled to thit surely as the sumple of the crish and they were all another of the frincion he sailed all the souler, as to sar with hom that had had seen hers will his alm this had and his\n",
      "farted. To the mord hod her heav that had boen to his servnce. Hilp high had\n",
      "brong hes one french they hore to him. The priscionatare the cammatter,\n",
      "shout to the rigit and shep and he could not see as he had seen his frinct, were aroughing.\n",
      "“Thou his feening was ne stranger. Ihe wan a conscious then they were insurvest to\n",
      "himself.\n",
      "\n",
      "“Ahat’s an ho so to the ment,\" said he, thating, timed tie paints with a man arigles\n",
      "on the ruming or her hasters of the\n",
      "foom.\n",
      "\n",
      "At the face, and their somper who wese to then, but warting that he cauld not such the postation while the orion and the pase who was\n",
      "not the farst story with trings to the charent of the pistens in the cringing\n",
      "or a prighted. His hand of arrence they were not ander he said and taken\n",
      "his frond ard thin had harr the poom tormid toet ther to the cours, the whole and the contersations and that shot at as in the chundry to a pistion on the comple she cart\n",
      "up the regaments anore and saried, having\n",
      "and who was a forent and same his harse on himself thit, sere his peated of tratens of the reart and\n",
      "stood at his sore of the forion of a fance and the\n",
      "cand that he saw the self, in the first of that his pass and arrude to the somplic soots ant this strunt of\n",
      "cressed,\n",
      "and she was tooking in the pirely and his pait toal were a contrate of her\n",
      "hume as the some who heard their castory, but he said that some too ald ho head was anowen in it was no bodith and\n",
      "this and a sermed on his himbres, truting, and than if the steriol of the\n",
      "rares throw that ha ded alrest that he had asked that he was no trought and her some the pooms, the same of that any while they were ilter and the part of tha prasoned of\n",
      "the faintlior was an ond the past of tho come in all with the provon string his persence on and\n",
      "and hiltered toe andry on the resolised and this was a solenge for\n",
      "which he cad been down to ansomait of some to his focks\n",
      "in the surpect or\n",
      "the paint tianding and set ale he saw teer his fasten and she\n",
      "and watched tootinestion.\n",
      "\n",
      "Tue taild while was a sourd after stousing a paint of the sare toot, becket that the wisher and sare at it was no time well of the room. But she dostened in to beene at he had arded ther while he waught ant too was allover of her fited onth her, so trat he soomed tie pates was\n",
      "that too was now she heal had, and his hastated, as then was sor them in her\n",
      "farted of are, that the stupp of the strengers of shont the whole on the crosh wete at that her arry of har to berin are through the some or their places, breen intendured, and tho couseron\n",
      "asten him and had there, and sole as a suntenterincly of a cold of\n",
      "the sume and shally to the caspon the pross thought they tork his sares of the feelt\n",
      "that seoming them to the pase ano she had trition to herself,\n",
      "because is saill was it he had to been and, and ansure aroned\n",
      "toming a coundres. Too taing and he had no arming it. That is he\n",
      "south ham a prisenes, and work onderstion.\n",
      "\n",
      "\"Y s you wint to be a per ant\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 10000, lstm_size, len(vocab), prime=\"Far Far Away\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i28000_l128.ckpt\n",
      "\n",
      "Far Far Away wife\n",
      "and thinky, wer her at a same, and wearly sent of a care of his hind the carnen of the\n",
      "cormed and sere and the downton of his at a sider that\n",
      "see at his fores the came ifterition at the cleather\n",
      "were that theres at the poot whit had a lad where\n",
      "the expectence wert inte the rass were tillesed\n",
      "him was sat that he had netted and the would some an to tale who had as stede her and har by\n",
      "this was stroted on hem tr there which the chouled the\n",
      "some talk ther as in the creet see a chard as he hid bo to him. The ding\n",
      "whare as this whone hindred to be the dindrers.\n",
      "\n",
      "The continion, she sad her all so were to settened\n",
      "with the soom and has beid with hus ware at was the rade. They hed\n",
      "not anye hes were to to her thought the cont were\n",
      "a firthofer of take that they would not to the bous had a fack of\n",
      "them was his arrosh has haps to her, the drowse of the ding and the wort tames affairs\n",
      "of the drewion of a still of her fere wear and so tho word.\n",
      "\n",
      "\"We dould have been at in?\" ssid Vomínivina as in \n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i28000_l128.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far Far Away\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i103850_l128.ckpt\n",
      "\n",
      "Far Far Away then in the same, bur in his pross,\n",
      "and stepplly so ald it aroused on his aroull, and he so the from whom he had been stooded him afr shipted that he was it all the\n",
      "come that teere was suprectents in the raciagian was\n",
      "sor to alring the sare.\n",
      "\n",
      "The staleron, and he walked his ald who had net happened ores and to hase as thin these\n",
      "could no mare to halp and sucret and the such heard he was though he was alling, thrye on\n",
      "the sirriant of him tho fellow, bot she ded to somare trat hos whole arm wores\n",
      "the consente attem or a most were in the stury and\n",
      "trind there was tho pert on a poors if and and had he did, and than saw one that in the reserouss of the\n",
      "frie souled his peovineds to she had been rished the\n",
      "ross there had sped them. Tean he were strent in a sunses\n",
      "the people and he whs to see his along and stole on her tare than the conssrion of thich as\n",
      "streled in him that he was aspece as the country of the some were horess to him as he wished to see\n",
      "the sour in a pirs, who was standing homp\n"
     ]
    }
   ],
   "source": [
    "checkpoint = checkpoint = 'checkpoints/i103850_l128.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far Far Away\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i62000_l128.ckpt\n",
      "\n",
      "Farthe said of shar of\n",
      "a soldrer, and and weal though he had aroust and staid as\n",
      "it was alling an the pirerelions was astooping\n",
      "at her fatis..\n",
      "\n",
      "“I should not tell yourself to his to batt to she then Ios if you should be streth any ale what wire it.. the comminte irm and in who came at,\n",
      "ard herself it whin then tro stairing of her fitten tie expecaion,\" and her wish hos had not seat to that\n",
      "trair of he whong his fach arong him to to his sore, was\n",
      "though in the rost that him almas and the foom. Tho sammon woo whared the\n",
      "soments sarions and hords and seeing, and she was suiled, and\n",
      "war stending to the forth of his hard we had answered at him.\n",
      "\n",
      "The sare to tho excrang which who had been in the strained of the possible.\n",
      "\n",
      "“Ah, yes, you know. In it an of you.”\n",
      "\n",
      "“Yes, I want hard, and at is a surpest that sat they, and see that is in as the montor of her facl of that a sound. It's ne still of the caust of that it ale there were taken. Hes mendened and was and that stared illow him instanced, and a\n"
     ]
    }
   ],
   "source": [
    "checkpoint = checkpoint = 'checkpoints/i62000_l128.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
